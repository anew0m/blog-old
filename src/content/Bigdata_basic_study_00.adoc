= 빅데이터 관련 학습노트 - 00 - 전체 정리
정민호
2020-05-09
:jbake-last_updated: 2020-05-10
:jbake-type: post
:jbake-status: published
:jbake-tags: 빅데이터, 데이터분석
:description: SK 동반성장아카데이 내 '머신러닝의 이해와 실습' 강의 정리
:jbake-og: {"image": "img/jdk/duke.jpg"}
:idprefix:
:toc:
:sectnums:



== 정리1. 머신러닝 개요와 데이터전처리
=== 머신러닝이란?
* 기계 즉 컴퓨터가 주변의 환경에서 발생하는 데이터를 학습하여 유의미한 패턴과 통계적인 함수를 발견하여 앞으로의 행동의 지침이 되는 지식을 얻어내는 행위

=== 머신러닝의 3가지 방법론
[horizontal]
감독학습:: target 값이 있는 데이터인 경우
비감독학습:: target 값이 없는 데이터인 경우
강화학습:: 게임의 agent가 스스로 정답이나 목표를 찾아가도록 학습하는 과정

=== 머신러닝 알고리즘의 범주
* 회귀
* 분류
* 추천
* 대체

=== 데이터 전처리 작업
* 데이터 멍밍(Munging)
* 랭글링(Wrangling)
* Missing Data 처리
* Outlier 처리 등




== 정리2. 의사결정트리(Decision Tree)
=== 분류 문제의 사용
* 분류 문제를 풀기 위해 Tree 모양의 Map을 만드는 것이 효과적임
* 나무 모양을 만들어내는 알고리즘을 의사결정트리라고 함

=== 엔트로피의 이해
* 의사결정트리를 만드는 과정
** 엔트로피가 가장 많이 줄어드는 방법으로 데이터를 나눔
** 다른 속성 기준으로 다시 데이터를 나눔
** 엔트로피가 0이 될 때까지 과정을 반복함
* 정보엔트로피를 구하는 공식 : asciimath:[sum_{i=1}^{n}p(x_{i})log_{2}p(x_{i})]

=== 의사결정트리
* 의사결정트리 알고리즘의 특징
** 엔트로피 이용
** 지니계수 이용
** 데이터를 나누는 방법에 대한 고민
** 가장 영향력이 큰 속성을 찾는 방법

* 카이제곱스퀘어 검정의 용도
** 2개 범주의 데이터를 나눌 때 상관관계가 0에 가까운지 검정하는 '독립성 검정'의 용도로 쓰임


== 정리 3. 회귀분석(Regression Analysis)
=== 비용함수를 표현하는 공식
* Cost Function 공식 : asciimath:[sum_{i}(y-y_{i})^{2}]

=== 회귀모형의 종류
* 단순회귀모형
* 다중회귀모형
* 선형회귀모형
* 비선형회귀모형

=== 결정계수
* 회귀모형이 현실데이터를 얼마나 잘 설명하는지 평가하기 위해 도입된 지수
* 0 에서 1 사이의 값

=== 회귀모형의 신뢰도 평가
* F값, P-value 결정계수 등의 변수 값으로 평가


== 정리 4. 군집분석(Clustering Analysis)
=== 군집분석의 종류
* 덴드로그램(Dendrogram)
* K-means(K 평균군집)
* DBSCAN(Density-Based 군집)

=== 계층형과 비계층형 군집
* 계층형
** 가장 가까운 거리의 데이터부터 차례대로 그룹을 이루어 나가면서 최종적으로 하나의 그룹으로 합치는 방식으로 트리 구조를 만드는 방법
** 종류 : 덴드로그램

* 비계층형
** 계층형이 아닌 방법
** 종류 : K-means 또는 DBSCAN

=== K-means의 단점
* 군집이 원의 형태(또는 구)에서 많이 벗어난 경우에는 (예: 길쭉한 형태) 오차가 많이 생기게 됨
* 밀도기반인 군집분석인 DBSCAN을 쓰면 효과적임

=== 덴드로그램의 용도와 단점
* 본격적인 군집분석을 하기 전에 대략적인 데이터의 패턴을 보고자 할 때 쓰임
* 메모리를 많이 쓰는 경향이 있어서 컴퓨터가 느려짐


== 정리 5. KNN(K-Nearest Neighbor)
=== KNN의 공식
* asciimath:[y = argmax_{v} \sum_{D_{x}}I(v=y_{i})]
* KNN의 공식 X를 중심으로 하는 데이터 집합 latexmath:[$D_{x}$] 에서 가장 많은 Lavel을 가지고 있는 v 값을 찾음

=== Weighted - KNN의 공식
* asciimath:[$y = argmax_{v} \sum_{D_{x}}I(v = y_{i}) , w_{i} = {1}/{d(x,x_{i})^{2}}]
* Unknown lavel에서 거리의 제곱의 역수를 가중치로 하여 거리가 멀수로 가중치를 떨어뜨리도록 모델을 설계함

=== KNN의 종류
* KNN Classifier와 KNN Regressor
* KNN Classifier : 범주현 데이터 예측
* KNN Regressor : 연속값 예측
* target 값에 따라 사용하는 모델이 달라짐
* 옵션은 Weighted의 기능 여부에 따라 4가지로 분류됨

=== 데이터 정류화
* 데이터 속성들을 하나의 scale로 통일해야 함
* 가장 많이 쓰이는 방법은 통계학에서 쓰이는 z정규화 방식임
* asciimath:[z_{i} = {x_{i} - avg(x)} / {sigma} , sigma]는 표준편자 asciimath:[avg(x)]는 x평균


== 정리 6. 나이브 베이즈(Naive Bayes)
=== 베이즈 이론식
* P(A) * P(B|A) = P(B) * P(A|B)

=== 나이브 베이즈 알고리즘
* P(특정단어)가 나타날 확률은 일일이 구할 수 없으므로 모두 같다고 생각함
* X문서 = {'단어1', '단어2', '단어3', ... } 일 때에 P(X|스팸)인 확률은 서로 독립이라고 가정하고,
단순하게 P('단어1'|스팸) * P('단어2'|스팸) * P('단어3'|스팸) * ... 으로 계산함

=== 나이브 베이즈의 적용
* 콜센터의 상담전화가 걸려 올 떄에 쓰는 단어들 목록을 보고 유추하여 해당하는 전문상담원을 연결시켜 주는 모델임
* 결혼을 준비 중인 30대 커플들의 선호하느 ㄴ신혼가구들의 분포를 예측하는 모델임

=== 감성분석
* 영화 댓글을 판별하여 긍정/부정을 예측하는 모델을 말함
* 텍스트 마이닝의 영역에서 '감정분석'은 나아가 NLP(자연어 처리)까지 발전될 수도 있음


== 정리 7. SVM(Support Vector Machine)
=== Hyper Plane
* 2개의 서로 다른 Class를 구분하는 경계면

=== Support Vector(지지 벡터)
* Hyper Plane에서 최대한 평행으로 양쪽으로 떨어지면서 가장 먼저 데이터와 만나는 지점을 통화하는 벡터

=== 커널 함수
asciimath:[K(x_{i}, x_{j}) = Phi(x_{i})^{T} Phi(x_{j})]

=== SVM(Support Vector Machine)
* 선형여부
** 선형 SVM
** 비선형 SVM

* target 데이터 종류
** 분류기를 만드는 SVM
** 회귀모형을 만드는 SVR


== 정리 8. 텍스트 마이닝(Text mining)
=== TF-IDF의 수식
* asciimath:[TF_{ij} ** IDF_{i} = TFIDF_{ij}]

=== TF와 IDF
* TF
** 단어의 빈도수이고 해당 문서에서 해당 단어가 나타나는 비율

* IDF
** 역문서 빈도로서 전체 문서에서 해당 단어가 나타나는 문서의 비율의 역수에 log를 취한 값

=== 한글 현태소 분석기
* Twitter
* Komoran
* 꼬꼬마 등

=== 텍스트 추출 라이브러리
* html 이나 xml을 파싱하여 순수한 텍스트를 추출함 - BeautifulSoup 등


== 정리 9. 주성분 분석(PCA)과 밀도기반 군집분석(DBSCAN)
=== PCA의 특징
* 새로운 축은 독립이며 직각임
* 원본 데이터의 차원이 p라면 새롭게 만들어진 데이터의 차원은 k(<p)
* 원본 데이터 X는 U와 V의 곱으로 분리됨
* U는 데이터를 새로운 차원 k로 설명함
* V는 원본 차원과 축소 차원의 관계를 설명함
* 정보를 많이 잃어버리지 않고 차원을 축소시킴

=== DBSCAN 용어
[horizontal]
이웃 벡터:: 반경 asciimath:[epsilon] 안에 포함된 데이터들
핵심 벡터:: n개 이상의 이웃 벡터를 가짐
직접 접근 기능:: 핵심 벡터와 이웃 벡터와의 관계(p → q)
접근 가능:: 연속적으로 이루어짐
* 핵심 벡터 → 이웃 벡터 → 이웃 벡터 → 이웃 벡터 → ... 일 때에 접근 가능으로 표현 (p ⇒ q)
* 연결된 p와 q사이에 접근 가능한 벡터가 있었다면 p ⇔ q 로 표현

=== DBSCAN vs k-means
* 밀도기반 vs 거리기반
* 어떤 형태의 군집도 잘 잡는 편 vs 원이나 구 모양에 최적화되어 있음
* 노이즈가 정의됨 vs 노이즈가 정의가 안됨
* 직관적 vs 수학적
* 프로그래밍으로 구현 vs 컴퓨터가 없어도 계산으로 풀 수 있음

=== PCA에서의 새로운 차원 k
* 고윳값 분해를 하여 가장 작으면서 분산을 많이 설명할 수 있는 상위 k개의 추상적인 축을 선택


== 정리 10. 신경망(Neural Network)
=== 신경망의 특징
* 신경세포인 뉴런의 동작을 모방함
* 마빈 민스키에 의해 개발됨
* 플랑크 로젠블랑의 이론임
* 신경망의 가중치 행렬에 대한 최적해를 구해야 함
* Gradient Descent를 사용함
* Gradient Descent를 구하기 위하여 Back Propagation(오류 역전파) 알고리즘을 사용함

=== Gradient Descent 수식
* asciimath:[W_{t + 1} larr W_{t} - lambda ** {delE(W_{t})} / {del W_{t}}]
* asciimath:[lambda]는 학습률
* W행렬은 W에 대한 에러함수의 변화율만큼 움직이면서 W를 갱신함

=== 신경망의 단점
* 층이 깊어지면 W가 0에 가까워짐
** Anish효과나 Explode효과의 발생으로 훈련이 제대로 이루어지지 않음
* 오버피팅(Overfitting)이 생김


== 정리 11. Word2Vec
=== Word2Vec의 특징
* 단어를 벡터로 취급함
* 단어 사이의 거리와 방향까지 알 수 있음
* 단어 임베딩의 size를 정함
* 많은 정보를 보여줌
* 더 확장하여 텍스트 간의 거리를 구하는 방법이 연구됨
* TF-IDF나 BagOfWords의 한계를 극복함

=== Skip Gram 알고리즘
* 단어의 주변에 나타나는 단어가 무엇인지 예측하는 신경망
* 최종적으로 나타나는 가중치 행렬은 해당 단어를 나타내는 임베딩 벡터의 모임

=== Skip Gram 변수
* V : 사전의 크기로서 전체 단어들의 개수
* N : 단어를 표현할 임베딩 벡터의 크기로서 신경망에서 hidden layer의 size
* Window Size : 주변에 나타나는 단어를 선정할 때 반경


== 정리 12. 토픽 모델링(Topic Modeling)
=== 토픽 모델링의 개요
* 구조화되지 않는 대량의 텍스트로부터 숨겨져 있는 주제구조를 발견하기 위한 통계적 추론 알고리즘

=== LDA의 개요
* 문서 같은 데이터의 집합에 대한 Generative Probabilistic Model(생성적 확률모델)

. Choose N ~ Possion(asciimath:[xi]).
. Choose asciimath:[theta] ~ Dir(asciimath:[alpha]).
. For each of the N words asciimath:[W_{t}]
.. Choose a topic asciimath:[Z_{n}]~Multinomial(asciimath:[theta)]
.. Choose a word asciimath:[W_{n}] from (asciimath:[W_{n} | Z_{n}, beta]),
a mutinomial probability conditioned on the topic asciimath:[Z_{n}].

=== 토픽 모델링의 주요변수
* asciimath:[beta_{ik}] : 단어 사전에서 i번째 단어가 k번째 주제에 해당할 확률
* asciimath:[W_{ik}] : i번째 단어이면서 k번째 주제에 해당하는 단어
* asciimath:[Z_{ik}] : i번째 단어의 k번째의 주제
* asciimath:[theta] : 디리클레 분포에서 추출되는 차원 k를 갖는 주체벡터
* asciimath:[k] : 주체(토픽)의 개수
* asciimath:[N] : 문서의 길이


== 정리 13. 랜덤 포레스트(Random Forest)와 에이다부스트(AdaBoost)
=== 배깅이란?
* 훈련데이터에서 중복을 허용하여 여러 표본그룹으로 분할하고 각각의 학습 데이터 그룹마다 약한 학습기를 생성하는 방법
* 결과를 취합할 때에는 다수결로 하거나 평균을 냄

=== 부스팅이란?
* 가중치에 따라서 학습의 강도를 결정하거나 표본데이터의 크기도 변경하여 애매한 결과가 나오는 모델이나 데이터에 더 집중할 수 있도록 가중치를 변화시키는 방법
* 결과를 취합할 때도 가중치 평균이나 가중치를 투표함

=== 랜덤 포레스트의 개요
* '배깅'의 일종으로 약한 학습기를 '결정트리'로 만드는 경우의 모델임

=== 에이다부스트의 개요
* '부스팅'의 일종으로 난이도가 높거나 오류율이 높은 데이터를 제대로 분류할 수 있도록 약한 핛ㅂ기마다 가중치에 변화를 주어 정확도를 높이는 방법
* 오류율이 높은 데이터는 더 큰 확률로 Resampling되도록 설계


== 정리14. 소셜 네트워크 분석(Social Network Analysis)
=== SNA의 개요
* 네트워크 및 그래프 이론을 사용하여 사회구조를 분석하는 머신러닝의 한 분야임

=== 중심성 지수의 정리
* 근접 중심성
* 중개 중심성
* 아이겐벡터(고유벡터) 중심성

=== 소셜 네트워크 분석의 적용분야
* 커뮤니케이션
* 사회심리학
* 정치
* 조직학
* 지리학 등

=== 그래프의 요소
* 노트와 엣지(연결선)

=== 그래프의 종류
* 방향 그래프와 무방향 그래프


== 정리 15. 랜덤 포레스트, 나이브 베이즈, Tf-Idf, Word2Vec
=== 나이브 베이즈 + Tf-idf의 머징 모델
* Tf-idf의 특성변수가 나이브 베이즈의 입력변수로 들어간 머징 모델임

=== 랜덤 포레스트 + Tf-idf의 머징 모델
* Tf-idf의 특성변수가 랜덤 포레스트의 입력변수로 들어간 머징 모델임

=== 랜덤 포레스트 + Word2Vec(doc2vec)
* 텍스트의 특성벡터를 Word2Vec으로 평균을 내어 계산한 다음 이것을 랜덤 포레스트에 입력변수로 넣어서 만든 머징 모델임


== 정리 16. 유전자 알고리즘(Genetic Algorithm)
=== 유전자 알고리즘
* 생물학적 진화와 자연선택의 기본원리에 영감을 얻은 확률적 탐색 알고리즘
* '자연선택', '염색체 교배', '교차', '변이'와 같은 생물학적 매카니즘을 모방함

=== 교차와 변이
* 교차
** 부모염색체가 끊어지는 부분(교차점)을 임의로 선택하고 2개의 부모염색체를 교차시켜서 연결하여 새로운 자식염색체를 만드는 과정

* 변이
** 부모염색체의 임의의 셀 부분이 다른 값으로 바뀌어 새로운 자식염색체를 만드는 과정
** 낮은 확률로 허용하는 보조연산자와 같은 기능을 함으로써 지역최적화를 방지하는 기능

=== 인코딩과 디코딩
* 인코딩
** 문제를 잘 파악하여 구하고자 하는 솔루션의 format을 염색체 형태로 만드는 방법

* 디코딩
** 적합도를 구하기 위하여 현실적인 문제 영역으로 보여주는 부분

=== 적합도 함수
* 문제의 규칙과 제약조건을 잘 정리하여 솔루션(해)이 가져야 하는
상점과 벌점을 중요순위대로 부과하여 전체적인 해의 score를 반환하도록 설계함
* 새로운 해가 환경에 얼마나 잘 적응을 하는지 점수화를 하는 부분임


== 정리 17. 연관규칙분석(Association Rule Analysis)
=== 지지도(Support)와 신뢰도(Confidence)
* 지지도 : 상품 X와 Y를 동시에 구입한 비율
* 신뢰도 : 상품 X를 샀을 때에 상품 Y도 같이 구입된 비율
* s(X => Y) = support(지지도) = n(X U Y) / N = P(X U Y)
* c(X => Y) = confidence(신뢰도) = n(X U Y) / n(X) = P(Y | X)

=== 향상도(Lift)
* 전체 트랜잭션 중에서 물품 Y를 구매한 비율에 비해서 물품 X를 구매했을 때에 Y를 구매한 비율을 비교하여 얼마나 향상되었는지 보는 것

-> 1이면 아무 의미가 없고 절대값이 1도가 클수록 의미가 있는 규칙으로 추출됨

* Lift(A, B) = asciimath:[{c(A->B)} / {s(B)}]

=== 어프라이어리 방법론
* 빈발항목 집합을 찾아내는 방법
** 한 항목집합이 빈발 : 그 집합의 모든 부분집합은 빈발항목집합
** 한 항목집합이 비빈발 : 그 항목집합을 포함하는 모든 집합은 비빈발항목집합


== 정리 18. 로지스틱 회귀분석(Logistic Regression Analysis)
=== 로지스틱 회귀분석
* 결과값이 P(Y = 1 | X)을 예측하기 때문에 2개의 class를 분류하는 문제에 쓰임
* 확장하면 Multi class도 가능함

=== Odds Ratio
* class가 0과 1이 있다면 1일 확률과 0일 확률의 비를 의미한
* asciimath:[{P(Y=1|x)} / {1-P(Y=1|x)}] = Odds Ratio

=== Odds Ratio를 이용한 로지스틱 회귀분석
* dds Ratio에 log을 씌운 값이 일반 회귀분석과 같은 수식이 됨
* ln asciimath:[{P(Y=1|x)} / {1-P(Y=1|x)} = w^{t}x + b]

=== 인코딩과 디코딩
* 인코딩
** 구하고자 하는 솔루션의 format을 염색체 형태로 만드는 방법
* 디코딩
** 적합도를 구하기 위하여 현실적인 문제 영역으로 보여주는 부분

=== 적합도 함수
* 문제의 규칙과 제약조건을 잘 정리하여 솔루션(해)이 가져야 하는 상점과 벌점을 중요순위대로 부과함
* 전체적인 해의 score를 반환하도록 설계함
* 새로운 해가 환경에 얼마나 잘 적응을 하는지 점수화를 하는 부분임


== 정리 19. 시계열분석(Time Series Analysis)
=== 시계열 패턴의 구성요소
* 추세, 잡음, 계절성
* asciimath:[x_{t} = T_{t} + S_{t} + bb"a"_{t}, bb"a"_{t} ~ N(0, sigma)]

=== 자기상관함수와 교차상관함수
* 자기상관함수
** 같은 변수의 시계열에서 서로 다른 시간대의 데이터끼리 상관관계를 설명함
* 교차상관함수
** 다른 변수의 다른 시계열끼리의 상관관계를 설명함

=== ARIMA의 부분모형
* AR(자기회귀모형), I(누적모형), MA(이동평균모형)

=== 정상성의 조건
* 모든 시점 t에 대하여 평균이 일정하다. asciimath:[E(Z_{t}) = u]
* 분산 Var asciimath:[(Z_{t})]는 시전 t에 의존하지 않는다.
* 두 시점 t, s에서 공분산 Cov asciimath:[(Z_{t} Z_{s})]는 시차 t - s에 의존하지 않는다.


== 정리 20. 모델 평가 방법 및 심화 이론
=== ARMA 모델
* 자기회귀이동평균모형(Auto Regression Movig Average model, ARMA)
* AR(자기회귀) 모델과 MA(이동평균) 모델이 합쳐진 합성모델임
* asciimath:[Z_{t} = Phi_{1}Z_{t-1} + Phi_{2}Z_{t-2} + ... + Phi_{p}Z_{t-p} + ... + a_{t} - theta_{1}a_{t-1} - theta_{2}a_{t-2} - theta_{3}a_{t-3} - ... -  - theta_{q}a_{t-q}, a_{t}]

=white nosie ~ asciimath:[N(0, sigma^{2})]

=== Gradient descent란?
* asciimath:[W^{(tau + 1)} = W^{(tau)} - eta grad E(W^{(tau)})]

=== Layer j의 delta
* asciimath:[delta_{j} = {del E_{n}} / {del Z_{j}}]

=== ROC 챠트
* x축을 FP rate로 하고 y축을 TP rate로 하여 곡선을 그림
* Curve의 밑면적을 AUC라고 하는데 이 면적이 넓을수록 신뢰성이 강한 모델임
* Curve의 모양이 좌측 상단으로 치우칠수록 좋은 모델임